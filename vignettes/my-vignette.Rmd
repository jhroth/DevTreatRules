---
title: "Using DevTreatRules"
#author: "Jeremy Roth"
#date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{DevTreatRules}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "##"
)
library(DevTreatRules)
```
Here, we illustrate the `DevTreatRules` package by building and evaluating treatment rules based on the the example dataset included with the package.
```{r}
head(example_df)
```

## Split the dataset
We can split the `example` dataset included in `DevTreatRules` by calling the `SplitData()` function
```{r split_data}
set.seed(123)
example.split <- SplitData(data=example_df, n.sets=3, split.proportions=c(0.5, 0.25, 0.25))
table(example.split$partition)
```
and forming independent development/validation/evaluation subsets based on the `partition` variable created above.
```{r make_subsets, message=FALSE}
library(dplyr)
development.data <- example.split %>% filter(partition == "development")
validation.data <-  example.split %>% filter(partition == "validation")
evaluation.data <-  example.split %>% filter(partition == "evaluation")
```
## Decide on variable roles
Suppose we believe the variables `prognosis`, `clinic`, and `age` may have influenced treatment assignment. We would codify this knowledge into `DevTreatRules` by specifying the argument
```{r}
names.influencing.treatment=c("prognosis", "clinic", "age")
```
in functions we will call later in this vignette. Further suppose that we don't expect `prognosis` and `clinic` to be measured on the same scale in independent clinical settings where we would like to apply our estimated rule (so they are not sensible rule inputs). However, we do expect the gene expression measurements in `gene_1`, ..., `gene_10` to potentially influence treatment response and also to be reliably measured in future settings (so they are sensible rule inputs). We will codify this in `DevTreatRules` with the argument
```{r}
names.influencing.rule=c("age", paste0("gene_", 1:10))
```

## On the development dataset, build the treatment rule
Although we could directly estimate a single treatment rule on the development dataset with the BuildRule function
```{r build_one_rule}
one.rule <- BuildRule(data=development.data,
                      study.design="observational",
                      prediction.approach="split.regression",
                      name.outcome="no_relapse",
                      type.outcome="binary",
                      desirable.outcome=TRUE,
                      name.treatment="intervention",
                      names.influencing.treatment=c("prognosis", "clinic", "age"),
                      names.influencing.rule=c("age", paste0("gene_", 1:10)),
                      propensity.method="logistic.regression",
                      rule.method="glm.regression")
```
this has limited utility because it required us to specify just one approach (even if we don't have a priori knowledge about which of split-regression, OWL framework, and direct-interactions approaches will perform best) and to specify just one regresion method for the `propensity.score` and `rule.method` arguments (even if we are not sure whether standard GLM or penalized GLM that performs variable shrinkage/seletion will yield a better rule).

In view of this, it would be more useful to perform model selection to estimate rules for all combinations of split-regression/OWL framework/direct-interactions and standard/lasso/ridge logistic regression (e.g. by looping over calls to `BuildRule()`). The model-selection process is automated with the function BuildRulesOnValidation.

## On the validation dataset, perform model selection
Here we will perform variable selection by calling `BuildRulesOnValidation()` with the arguments
```{r}
vec.approaches=c("split-regression", "OWL.framework", "direct.interactions")
vec.rule.methods=c("glm.regression", "lasso", "ridge")
vec.propensity.methods=c("logistic.regression", "lasso", "ridge")
```
that reflect the modeling choices currently supported by `DevTreatRules`, and are actually the default values of the arguments in `BuildRulesOnValidation()`.
```{r model_selection_on_validation}
print("BuildRulesOnValidation")
```
So it looks like NEED TO ADD and NEED TO ADD might perform the best. We will lock these in and apply them to the evaluation set

## On the evaluation dataset, evaluate the selected treatment rules
```{r evaluate_selected_treatment_rules_on_test_set}

```
The above estimates, for the locked-in treatment rules on the evaluation set, are what we'd expect to observe in an independent clinical setting where individuals are drawn from the same population.

We could have also obtain bootstrap-based CIs for the ATE/ABR estimates (in both the validation and evaluation datasets) by specifying the arguments to `BuildRulesOnValidation` and `EvaluatRule()`
```{r}
bootstrap.CI=TRUE
booststrap.CI.replications=1000
```
(or for any desired number of replications) but we chose not to compute the CIs in this example to minimize run-time.
